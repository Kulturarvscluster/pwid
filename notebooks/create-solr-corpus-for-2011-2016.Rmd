---
title: "Create Solr coprpus"
author: pmd
date: 2021-03-31
output: html_notebook
---

```{r libs}
library(dplyr)
library(sparklyr)
library(here)
```

```{r custom_functions}
source(here::here("src/spark_util.R"))
```

```{r connect}
conf <- spark_config()

# brug max 40 executors. Det er ca. 3/4 af clusteret og vil derfor lade os lave andet
conf$spark.dynamicAllocation.maxExecutors <- 40
conf$spark.sql.files.maxPartitionBytes = "2147483647" 
# conf$spark.sql.files.openCostInBytes="2147483647"

# This will hopefully allow the job to work through heavy loads. Default is 30, which caused
#
# 9/04/09 13:25:35 ERROR TransportClientFactory: Exception while bootstrapping client after 30002 ms
# java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timeout waiting for task.
# at org.spark_project.guava.base.Throwables.propagate(Throwables.java:160)
# at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:275)
# at org.apache.spark.network.sasl.SaslClientBootstrap.doBootstrap(SaslClientBootstrap.java:70)
# at org.apache.spark.network.crypto.AuthClientBootstrap.doSaslAuth(AuthClientBootstrap.java:115) # at org.apache.spark.network.crypto.AuthClientBootstrap.doBootstrap(AuthClientBootstrap.java:74) conf$spark.shuffle.sasl.timeout <- '300s'
#
# Should prevent the
# https://community.hortonworks.com/content/supportkb/186413/errorjavaioioexception-filesystem- closed-when-runn.html

conf$fs.hdfs.impl.disable.cache <- TRUE
conf$fs.file.impl.disable.cache <- TRUE
```


# Kode til at skabe Solr korpus


```{r}
chosen_year <- 2012
source(here::here("config-data/paths.R"))
log_prefix <- chosen_year
log("Starting")

spark_disconnect_all()
sc <-
   spark_connect(
      master = 'yarn-client',
      app_name = paste("mergeSolrWithCorpusNotebook"),
      config = conf
   )
log("Spark reconnected")

spark_get_checkpoint_dir(sc)
if (spark_get_checkpoint_dir(sc) != "") {
   existing_checkpoint <- spark_get_checkpoint_dir(sc)
   existing_checkpoint <-
      sub(paste0("hdfs://KAC/user/", Sys.getenv("USER"), "/"),
          "",
          existing_checkpoint)
   existing_checkpoint
   hdfs_delete_skipTrash(existing_checkpoint)
}

spark_set_checkpoint_dir(sc, "spark_checkpoints")
spark_get_checkpoint_dir(sc)
log_prefix <- corpus_repartitioned_path

tryCatch(
   hdfs.file.info(path = paste0(corpus_repartitioned_path, "/_SUCCESS")),
   error = function(e) {
      print(e$message) # or whatever error handling code you want
      
      log("Loading corpus")
      corpus <- spark_read_parquet(
         sc,
         name = corpus_name,
         path = corpus_path,
         memory = FALSE,
         overwrite = TRUE
      ) %>%
         #filter(annotations=='le:IOException@ExtractorSWF,duplicate:"3955-27-20060312001849-00174- sb-prod-har-001.statsbiblioteket.dk.arc,65003333",content-size:12389') %>%
         #Create the year var instead of using corpus_id as year
      mutate(crawl_year = corpus_id) %>% 
      mutate(
         #Save the orig values
         orig_crawl_year = crawl_year,
         orig_harvest_id = harvest_id,
         orig_job_id = job_id
      ) %>%
         #Handle duplicate annotation
      mutate(dedup = instr(annotations, "duplicate:") > 0) %>%
      mutate(annotations_duplications = regexp_extract(annotations, 'duplicate:"([^"]*)"', 1)) %>% 
      mutate(annotations_list = split(annotations_duplications, '-')) %>%
      sdf_separate_column(
         column = "annotations_list",
         into = c("dedup_job_id",
                  "dedup_harvest_id", "dedup_date")
      ) %>%
      select(-annotations_list,-annotations_duplications) %>%
      mutate(dedup_crawl_year = substring(dedup_date, 0, 4)) %>% #First four chars 
      mutate(
         crawl_year = ifelse(dedup, dedup_crawl_year, orig_crawl_year),
         harvest_id = ifelse(dedup, dedup_harvest_id, orig_harvest_id), 
         job_id = ifelse(dedup, dedup_job_id, orig_job_id),
         ) %>% #replace real values with values from duplicate, if the record is a duplication 
      select(-dedup_crawl_year,-dedup_harvest_id,-dedup_job_id,-dedup_date) %>% #Remove the extra duplication columns
      mutate(sha1 = regexp_replace(sha1, "^[^:]+:", "")) %>% #SOLR SHA1 start with sha1:, corpus does not... (sometimes)
      sdf_repartition(
         partitions = 1008,
      
         partition_by =  c(
            "corpus_id",
            "crawl_year",
            "harvest_id",
            "job_id",
            "status_code",
            "size",
            "uri",
            "sha1"
      )
   )
   log("Writing ", corpus_repartitioned_path)
   
   spark_write_parquet(corpus, path = corpus_repartitioned_path)
   log("Written ", corpus_repartitioned_path)
   }
)

log("Reading ", corpus_repartitioned_path)
corpus <- spark_read_parquet(
   sc,
   name = corpus_name,
   path = corpus_repartitioned_path,
   repartition = 0,
   memory = FALSE,
   overwrite = TRUE
)


# TODO Do the below sutff for this and every preceding year of the crawl-log and concatenate the results

log("Starting solr/corpus join")
log_prefix <- solr_corpus_keys_path
#NEW PLAN
# Only include the overall join-keys (corpus_id, crawl_year, harvest_id, job_id)
# from corpus. Then semi_join this on the solr DF to filter out all the irrelevant lines. # Hopefully, afterwards the solr DF will be of a more manageable size
tryCatch(
   hdfs.file.info(path = paste0(solr_corpus_keys_path, "/_SUCCESS")) ,
   error = function(e) {
      corpus_keys <- corpus %>%
         select(corpus_id, crawl_year, harvest_id, job_id) %>% distinct() %>%
         sdf_register("corpus_keys")
      
      log("Reading ", solr_path)
      solr <- spark_read_parquet(
         sc,
         name = solr_name,
         path = solr_path,
         memory = FALSE,
         overwrite = TRUE
      ) %>% sdf_register(solr_name)
      
      #Filter out the irrelevant entries
      solr_corpus_keys <- solr %>%
         semi_join(sdf_broadcast(corpus_keys))
      
      size_of_dataframe(solr_corpus_keys)
      
      #Write the resulting reduced SOLR DF
      # THIS MIGHT NOT BE NEEDED
      log("writing ", solr_corpus_keys_path)
      spark_write_parquet(solr_corpus_keys, path = solr_corpus_keys_path, mode = 'overwrite')
      log("Written ", solr_corpus_keys_path)
   }
)
log_prefix <- solr_corpus_without_dedup_path

# Done with the reduction of the Solr dump

# next: join corpus with reduced Solr dump

tryCatch(
   hdfs.file.info(path = paste0(solr_corpus_without_dedup_path, "/_SUCCESS")),
   error = function(e) {
      
      log("Reading ", solr_corpus_keys_path)
      solr_corpus_keys <- spark_read_parquet(
         sc,
         name = solr_name,
         path = solr_corpus_keys_path,
         memory = FALSE,
         overwrite = TRUE
      )
      solr_corpus <- solr_corpus_keys %>%
         inner_join(
            corpus,
            by = c(
               "corpus_id" = "corpus_id",
               "crawl_year" = "crawl_year",
               "harvest_id" = "harvest_id",
               "job_id" = "job_id",
               "status_code" = "status_code",
               "size" = "size",
               "uri" = "uri",
               "sha1" = "sha1"
            )
      )
      log("Writing ", solr_corpus_without_dedup_path)
      spark_write_parquet(solr_corpus, path = solr_corpus_without_dedup_path, mode = 'overwrite')
      log("Written ", solr_corpus_without_dedup_path)
      hdfs_delete_skipTrash(solr_corpus_keys_path)
   }
)
log_prefix <- chosen_year
```
