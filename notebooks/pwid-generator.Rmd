---
title: "Generating PWID"
output: html_notebook
---

```{r}
library(sparklyr)
library(dplyr)
library(lubridate)
library(here)
```

# PWID

Fra mail fra Eld.

Opskrift på corpus definition

For hvert år der skal laves corpus definition til

 * Find resultatfil(er) for som vedrører corpus

 * Lav en fil til corpus definition (def-fil)

 * Find persistent corpus identifier (corpus-id)

 * Aflæs tid når dette program kører (corpus-def-tid) på form YYYY-MM-DDThh:mm:ssZ

 * Tilføj følgende linjer til def-fil (blåt er tekst rødt er fundne værdi):

```
<collection>
<collection_id>corpus-id</collection_id>
<collection_date> corpus-def-tid </collection_date>

<contents>
           <part>urn:pwid:netarkivet.dk:elem-tid:part:elem-url</part>
</contents>
</collection>
```

elem-tid: crawl datetime
elem-url: crawl URL

# Create connection to the cluster


```{r}
source(here::here("src/spark_util.R"))
```

Vi har brug for at sætte tidszonen for senere at kunne udskrive crawl tidspunktet i UTC+0. Dette lærte Asger jeg jeg fra [denne SO](https://stackoverflow.com/a/52365367). Det viste sig også at kun `conf$spark.sql.session.timeZone <- "UTC"` er nødvendig.

```{r}
conf <- spark_config()
conf$spark.dynamicAllocation.minExecutors <- 20 # secures at least 20 executors
conf$spark.sql.session.timeZone <- "UTC"

spark_disconnect_all()
sc <- spark_connect(
   master = 'yarn-client',
   app_name = paste("pwidlist"),
   config = conf
)
```


# PoC for 2010

## Test load af /datapool/dk-web-solr/solr-corpus/2010_corpus_solr-parquet

```{r}
chosen_year <- 2010
corpus_name <- paste0("corpus", chosen_year)
corpus_path <- paste0("/datapool/dk-web-solr/solr-corpus/", chosen_year, "_corpus_solr-parquet")


solr_corpus <- spark_read_parquet(
   sc,
   name = corpus_name,
   path = corpus_path, # use corpus_path for 2006-2011, use corpus_repartitioned_path for 2012-2016
   memory = FALSE,
   overwrite = TRUE
)
```

```{r}
glimpse(solr_corpus)
```

```{r}
tally(solr_corpus)
```

## Looking into date conversion

```{r}
solr_corpus %>% head(20) %>% collect() -> solr_head
```

```{r}
solr_head %>% 
   select(unix_time, solr_crawl_date, solr_unix_time) %>% 
   mutate(
      unix_time_date = as_datetime(unix_time),
      solr_unix_time_date = as_datetime(solr_unix_time)) %>% 
   select(solr_crawl_date, solr_unix_time_date, unix_time_date)
```

Okay, now try the same in sparklyr

```{r}
solr_corpus %>% 
   mutate(pwid_date = from_unixtime(solr_unix_time,"yyyy-MM-dd'T'HH:mm:ss'Z'")) %>% 
   select(pwid_date, solr_crawl_date, everything()) %>% 
   head(5)
```

Så er der styr på dato!

pwid_date = from_unixtime(solr_unix_time,"yyyy-MM-dd'T'HH:mm:ss'Z'")

## Create PWID

```{r}
archive <- "netarkivet.dk"

solr_corpus %>% 
   mutate(pwid_datetime = from_unixtime(solr_unix_time,"yyyy-MM-dd'T'HH:mm:ss'Z'")) %>% 
   mutate(pwid = paste("urn","pwid", archive, pwid_datetime, "part", uri, sep = ":")) %>%
   select(pwid)  %>% 
   head(10) %>% 
   collect()
```


## Endelig beregning af PWID

```{r}
archive <- "netarkivet.dk"

sdf_pwid <- solr_corpus %>% 
   mutate(pwid_datetime = from_unixtime(solr_unix_time,"yyyy-MM-dd'T'HH:mm:ss'Z'")) %>% 
   mutate(pwid = paste("urn","pwid", archive, pwid_datetime, "part", uri, sep = ":")) %>%
   select(pwid) %>% 
   compute(name = "sdf_pwid") # caches in Spark memory
```

```{r}
glimpse(sdf_pwid)
```

Hvor mange PWID'er?

```{r}
tally(sdf_pwid)
```

## Arkiver som csv filer


```{r}
pwidlist_path <- paste0("/projects/p002/pwid/",chosen_year,"_pwidlist")

spark_write_csv(
   sdf_pwid %>% sdf_coalesce(1),
   path = pwidlist_path,
   mode = "overwrite",
   header = FALSE
)
```



```
[pmdp002@kac-proj-002 ~]$ hdfs dfs -ls /projects/p002/pwid/2010_pwidlist
Found 2 items
-rw-r-----   3 pmdp002 p002           0 2019-10-17 12:42 /projects/p002/pwid/2010_pwidlist/_SUCCESS
-rw-r-----   3 pmdp002 p002 70007641891 2019-10-17 12:42 /projects/p002/pwid/2010_pwidlist/part-00000-58c43393-8352-4a65-8681-ebb0c1efe80f-c000.csv
```

Dette tog 54 minutter og resulterede i 70GB (70007641891B) og vi har nu en film ed et PWID pr. linie. I denne fil er der repræsenteret 473.144.026 dokumenter. Hver PWID fylder derfor i gennemsnit 147.9626B og med et overslag på 4,9 milliarder dokumenter i dk-web korpus, giver den et lagerforbrug på ca. 720GB.


## test opslag

**Dette afsnit er ikke gennemført for F2021**

Find et "tilfældigt" opslag

```{r}
library(stringr)
solr_corpus %>% 
   head(10000) %>% 
   collect() %>% 
   filter(type == "Image") %>% 
   filter(str_detect(uri, "jpg")) %>% 
   filter(str_detect(uri, "groen")) %>% 
   select(sha1, uri) -> head_image;head_image
```

Okay, mit testbillede bliver 

| hash | CFHYU3COOM6PSSAEQJFVPR5PKQRFIIII                                    |
| uri  | http://www.dr.dk/skole/img/im/land_og_folk/im_152x114_groenland.jpg |


```{r}
solr_corpus %>% 
   filter(sha1 == "CFHYU3COOM6PSSAEQJFVPR5PKQRFIIII") %>% 
   mutate(
      pwid_datetime = from_unixtime(unix_timestamp(solr_crawl_date ,'EEE MMM d H:m:s z yyyy'), "YYYY-MM-dd'T'HH:mm:ss'Z'")) %>% 
   mutate(pwid = paste("urn","pwid", archive, pwid_datetime, "part", uri, sep = ":")) %>%
   select(pwid)
```

Denne URL

https://solrwb-stage.statsbiblioteket.dk:4000/solrwayback/services/pwid/web/urn:pwid:netarkivet.dk:2010-02-17T21:58:28Z:part:http://www.dr.dk/skole/img/im/land_og_folk/im_152x114_groenland.jpg

giver det korrekte billede!


# Konklusion

Vi kan nu relativt nemt generere PWID for dk-web for 2006 til 2010. For 2011 til 2016 skal der udføres noget ekstra arbejde.

Der er brug for ca. 700 til 800 GB til bevaring af den samlede mængde PWID.

Min ummidelbare vurdering er at det vil kræve en uges arbejde at gennemføre opgaven med at generere PWID for dk-web.

