---
title: "Generating PWID"
output: html_notebook
---

```{r}
library(here)
```

```{r}
2+2
```


# PWID

Fra mail fra Eld.

Opskrift på corpus definition

For hvert år der skal laves corpus definition til

 * Find resultatfil(er) for som vedrører corpus

 * Lav en fil til corpus definition (def-fil)

 * Find persistent corpus identifier (corpus-id)

 * Aflæs tid når dette program kører (corpus-def-tid) på form YYYY-MM-DDThh:mm:ssZ

 * Tilføj følgende linjer til def-fil (blåt er tekst rødt er fundne værdi):

```
<collection>
<collection_id>corpus-id</collection_id>
<collection_date> corpus-def-tid </collection_date>

<contents>
           <part>urn:pwid:netarkivet.dk:elem-tid:part:elem-url</part>
</contents>
</collection>
```

elem-tid: crawl datetime
elem-url: crawl URL

# Create connection to the cluster

```{r}
setwd("~/Probing_002")
```


```{r, eval=FALSE}
source("common/spark_util.R")
```

Vi har brug for at sætte tidszonen for senere at kunne udskrive crawl tidspunktet i UTC+0. Dette lærte Asger jeg jeg fra [denne SO](https://stackoverflow.com/a/52365367). Det viste sig også at kun `conf$spark.sql.session.timeZone <- "UTC"` er nødvendig.

```{r}
conf <- spark_config()
conf$spark.dynamicAllocation.minExecutors <- 20 # secures at least 20 executors
conf$spark.sql.session.timeZone <- "UTC"

spark_disconnect_all()
sc <- spark_connect(
   master = 'yarn-client',
   app_name = paste("pwidlist"),
   config = conf
)
```




# PoC for 2010

## Test load af /datapool/dk-web-solr/solr-corpus/2010_corpus_solr-parquet

```{r}
chosen_year <- 2010
corpus_name <- paste0("corpus", chosen_year)
corpus_path <- paste0("/datapool/dk-web-solr/solr-corpus/", chosen_year, "_corpus_solr-parquet")


solr_corpus <- spark_read_parquet(
   sc,
   name = corpus_name,
   path = corpus_path, # use corpus_path for 2006-2011, use corpus_repartitioned_path for 2012-2016
   memory = FALSE,
   overwrite = TRUE
)
```

```{r}
glimpse(solr_corpus)
```

```{r}
tally(solr_corpus)
```

## Create PWID

Datoformatet skal være `YYYY-MM-DDThh:mm:ssZ`, men er fx "Sat Feb 20 03:15:14 CET 2010". Så nu skal jeg lige lære at manipulerer datoer i Hive.

[Hive manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions)

Okay, dette base R kan klare datoerne:

```{r}
strptime("Sat Feb 20 03:15:14 CET 2010", "%a %b %d %H:%M:%S CET %Y")
```

Men kan den bruges i en Hive/sparklyr?

Nej, det kan den ikke. Jeg ender

```
Error in View : org.apache.spark.sql.AnalysisException: Undefined function: 'strptime'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 700
```

Så, jeg er tvunget til at få Hives dato-gymnastik til at fungere.


## forsøg med at sætte tidszone i Spark

```{r}
archive <- "netarkivet.dk"

solr_corpus %>% 
   head(10) %>% 
   mutate(pwid_datetime = from_unixtime(unix_timestamp(solr_crawl_date ,'EEE MMM d H:m:s z yyyy'), "YYYY-MM-dd'T'HH:mm:ss'Z'")) %>% 
   mutate(pwid = paste("urn","pwid", archive, pwid_datetime, "part", uri, sep = ":")) %>%
   select(pwid_datetime, solr_crawl_date) -> pwid_small;pwid_small
```

## Endelig beregning af PWID

```{r}
archive <- "netarkivet.dk"

sdf_pwid <- solr_corpus %>% 
   mutate(pwid_datetime = from_unixtime(unix_timestamp(solr_crawl_date ,'EEE MMM d H:m:s z yyyy'), "YYYY-MM-dd'T'HH:mm:ss'Z'")) %>% 
   mutate(pwid = paste("urn","pwid", archive, pwid_datetime, "part", uri, sep = ":")) %>%
   select(pwid) %>% 
   compute(name = "sdf_pwid") # caches in Spark memory
```

```{r}
glimpse(sdf_pwid)
```

Hvor mange PWID'er?

```{r}
tally(sdf_pwid)
```

## Arkiver som csv filer


```{r}
pwidlist_path <- paste0("/projects/p002/pwid/",chosen_year,"_pwidlist")

spark_write_csv(
   sdf_pwid %>% sdf_coalesce(1),
   path = pwidlist_path,
   mode = "overwrite",
   header = FALSE
)
```



```
[pmdp002@kac-proj-002 ~]$ hdfs dfs -ls /projects/p002/pwid/2010_pwidlist
Found 2 items
-rw-r-----   3 pmdp002 p002           0 2019-10-17 12:42 /projects/p002/pwid/2010_pwidlist/_SUCCESS
-rw-r-----   3 pmdp002 p002 70007641891 2019-10-17 12:42 /projects/p002/pwid/2010_pwidlist/part-00000-58c43393-8352-4a65-8681-ebb0c1efe80f-c000.csv
```

Dette tog 75 minutter og resulterede i 70007641891B og vi har nu en film ed et PWID pr. linie. Hver PWID fylder i gennemsnit 147.9626B og med et overslag på 4,9 milliarder dokumenter i dk-web korpus, giver den et lagerforbrug på ca. 720GB.


## test opslag

Find et "tilfældigt" opslag

```{r}
library(stringr)
solr_corpus %>% 
   head(10000) %>% 
   collect() %>% 
   filter(type == "Image") %>% 
   filter(str_detect(uri, "jpg")) %>% 
   filter(str_detect(uri, "groen")) %>% 
   select(sha1, uri) -> head_image;head_image
```

Okay, mit testbillede bliver 

| hash | CFHYU3COOM6PSSAEQJFVPR5PKQRFIIII                                    |
| uri  | http://www.dr.dk/skole/img/im/land_og_folk/im_152x114_groenland.jpg |


```{r}
solr_corpus %>% 
   filter(sha1 == "CFHYU3COOM6PSSAEQJFVPR5PKQRFIIII") %>% 
   mutate(
      pwid_datetime = from_unixtime(unix_timestamp(solr_crawl_date ,'EEE MMM d H:m:s z yyyy'), "YYYY-MM-dd'T'HH:mm:ss'Z'")) %>% 
   mutate(pwid = paste("urn","pwid", archive, pwid_datetime, "part", uri, sep = ":")) %>%
   select(pwid)
```

Denne URL

https://solrwb-stage.statsbiblioteket.dk:4000/solrwayback/services/pwid/web/urn:pwid:netarkivet.dk:2010-02-17T21:58:28Z:part:http://www.dr.dk/skole/img/im/land_og_folk/im_152x114_groenland.jpg

giver det korrekte billede!


# Konklusion

Vi kan nu relativt nemt generere PWID for dk-web for 2006 til 2010. For 2011 til 2016 skal der udføres noget ekstra arbejde.

Der er brug for ca. 700 til 800 GB til bevaring af den samlede mængde PWID.

Min ummidelbare vurdering er at det vil kræve en uges arbejde at gennemføre opgaven med at generere PWID for dk-web.


# Kode til at skabe Solr korpus

```{r}
#setwd("~/projects/etl2-netarkivet") source("../common/spark_util.R")

conf <- spark_config()
#Set at vi max bruger 40 executors. Det er ca. 3/4 af clusteret og vil #derfor lade os lave andet
conf$spark.dynamicAllocation.maxExecutors <- 40
conf$spark.sql.files.maxPartitionBytes = "2147483647" #conf$spark.sql.files.openCostInBytes="2147483647"

#This will hopefully allow the job to work through heavy loads. Default is 30, which caused
# 9/04/09 13:25:35 ERROR TransportClientFactory: Exception while bootstrapping client after 30002 ms
# java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timeout waiting for task.
# at org.spark_project.guava.base.Throwables.propagate(Throwables.java:160)
# at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:275)
# at org.apache.spark.network.sasl.SaslClientBootstrap.doBootstrap(SaslClientBootstrap.java:70)
# at org.apache.spark.network.crypto.AuthClientBootstrap.doSaslAuth(AuthClientBootstrap.java:115) # at org.apache.spark.network.crypto.AuthClientBootstrap.doBootstrap(AuthClientBootstrap.java:74) conf$spark.shuffle.sasl.timeout <- '300s'
#Should prevent the
#https://community.hortonworks.com/content/supportkb/186413/errorjavaioioexception-filesystem- closed-when-runn.html

conf$fs.hdfs.impl.disable.cache <- TRUE
conf$fs.file.impl.disable.cache <- TRUE
chosen_year <- 2012
source("config-data/paths.R")
log_prefix <- chosen_year
log("Starting")

spark_disconnect_all()
sc <-
   spark_connect(
      master = 'yarn-client',
      app_name = paste("mergeSolrWithCorpusNotebook"),
      config = conf
   )
log("Spark reconnected")

spark_get_checkpoint_dir(sc)
if (spark_get_checkpoint_dir(sc) != "") {
   existing_checkpoint <- spark_get_checkpoint_dir(sc)
   existing_checkpoint <-
      sub(paste0("hdfs://KAC/user/", Sys.getenv("USER"), "/"),
          "",
          existing_checkpoint)
   existing_checkpoint
   hdfs_delete_skipTrash(existing_checkpoint)
}

spark_set_checkpoint_dir(sc, "spark_checkpoints")
spark_get_checkpoint_dir(sc)
log_prefix <- corpus_repartitioned_path

tryCatch(
   hdfs.file.info(path = paste0(corpus_repartitioned_path, "/_SUCCESS")),
   error = function(e) {
      print(e$message) # or whatever error handling code you want
      
      log("Loading corpus")
      corpus <- spark_read_parquet(
         sc,
         name = corpus_name,
         path = corpus_path,
         memory = FALSE,
         overwrite = TRUE
      ) %>%
         #filter(annotations=='le:IOException@ExtractorSWF,duplicate:"3955-27-20060312001849-00174- sb-prod-har-001.statsbiblioteket.dk.arc,65003333",content-size:12389') %>%
         #Create the year var instead of using corpus_id as year
      mutate(crawl_year = corpus_id) %>% 
      mutate(
         #Save the orig values
         orig_crawl_year = crawl_year,
         orig_harvest_id = harvest_id,
         orig_job_id = job_id
      ) %>%
         #Handle duplicate annotation
      mutate(dedup = instr(annotations, "duplicate:") > 0) %>%
      mutate(annotations_duplications = regexp_extract(annotations, 'duplicate:"([^"]*)"', 1)) %>% 
      mutate(annotations_list = split(annotations_duplications, '-')) %>%
      sdf_separate_column(
         column = "annotations_list",
         into = c("dedup_job_id",
                  "dedup_harvest_id", "dedup_date")
      ) %>%
      select(-annotations_list,-annotations_duplications) %>%
      mutate(dedup_crawl_year = substring(dedup_date, 0, 4)) %>% #First four chars 
      mutate(
         crawl_year = ifelse(dedup, dedup_crawl_year, orig_crawl_year),
         harvest_id = ifelse(dedup, dedup_harvest_id, orig_harvest_id), 
         job_id = ifelse(dedup, dedup_job_id, orig_job_id),
         ) %>% #replace real values with values from duplicate, if the record is a duplication 
      select(-dedup_crawl_year,-dedup_harvest_id,-dedup_job_id,-dedup_date) %>% #Remove the extra duplication columns
      mutate(sha1 = regexp_replace(sha1, "^[^:]+:", "")) %>% #SOLR SHA1 start with sha1:, corpus does not... (sometimes)
      sdf_repartition(
         partitions = 1008,
      
         partition_by =  c(
            "corpus_id",
            "crawl_year",
            "harvest_id",
            "job_id",
            "status_code",
            "size",
            "uri",
            "sha1"
      )
   )
   log("Writing ", corpus_repartitioned_path)
   
   spark_write_parquet(corpus, path = corpus_repartitioned_path)
   log("Written ", corpus_repartitioned_path)
   }
)

log("Reading ", corpus_repartitioned_path)
corpus <- spark_read_parquet(
   sc,
   name = corpus_name,
   path = corpus_repartitioned_path,
   repartition = 0,
   memory = FALSE,
   overwrite = TRUE
)


# TODO Do the below sutff for this and every preceding year of the crawl-log and concatenate the results

log("Starting solr/corpus join")
log_prefix <- solr_corpus_keys_path
#NEW PLAN
# Only include the overall join-keys (corpus_id, crawl_year, harvest_id, job_id)
# from corpus. Then semi_join this on the solr DF to filter out all the irrelevant lines. # Hopefully, afterwards the solr DF will be of a more manageable size
tryCatch(
   hdfs.file.info(path = paste0(solr_corpus_keys_path, "/_SUCCESS")) ,
   error = function(e) {
      corpus_keys <- corpus %>%
         select(corpus_id, crawl_year, harvest_id, job_id) %>% distinct() %>%
         sdf_register("corpus_keys")
      
      log("Reading ", solr_path)
      solr <- spark_read_parquet(
         sc,
         name = solr_name,
         path = solr_path,
         memory = FALSE,
         overwrite = TRUE
      ) %>% sdf_register(solr_name)
      
      #Filter out the irrelevant entries
      solr_corpus_keys <- solr %>%
         semi_join(sdf_broadcast(corpus_keys))
      
      size_of_dataframe(solr_corpus_keys)
      
      #Write the resulting reduced SOLR DF
      # THIS MIGHT NOT BE NEEDED
      log("writing ", solr_corpus_keys_path)
      spark_write_parquet(solr_corpus_keys, path = solr_corpus_keys_path, mode = 'overwrite')
      log("Written ", solr_corpus_keys_path)
   }
)
log_prefix <- solr_corpus_without_dedup_path

# Done with the reduction of the Solr dump

# next: join corpus with reduced Solr dump

tryCatch(
   hdfs.file.info(path = paste0(solr_corpus_without_dedup_path, "/_SUCCESS")),
   error = function(e) {
      
      log("Reading ", solr_corpus_keys_path)
      solr_corpus_keys <- spark_read_parquet(
         sc,
         name = solr_name,
         path = solr_corpus_keys_path,
         memory = FALSE,
         overwrite = TRUE
      )
      solr_corpus <- solr_corpus_keys %>%
         inner_join(
            corpus,
            by = c(
               "corpus_id" = "corpus_id",
               "crawl_year" = "crawl_year",
               "harvest_id" = "harvest_id",
               "job_id" = "job_id",
               "status_code" = "status_code",
               "size" = "size",
               "uri" = "uri",
               "sha1" = "sha1"
            )
      )
      log("Writing ", solr_corpus_without_dedup_path)
      spark_write_parquet(solr_corpus, path = solr_corpus_without_dedup_path, mode = 'overwrite')
      log("Written ", solr_corpus_without_dedup_path)
      hdfs_delete_skipTrash(solr_corpus_keys_path)
   }
)
log_prefix <- chosen_year
```
